{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5329assn2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonnoy/COMP5329NEW/blob/jjh/5329assn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P22itkey3yuS"
      },
      "source": [
        "# mount google drive to colabif not mounted before\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbqypOO10hvP"
      },
      "source": [
        "# import necessary library\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "\n",
        "DEBUG = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61DjxixN3QLF"
      },
      "source": [
        "# download data from kaggle server with token file\n",
        "# must update before use in Colab!\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "# following method of download dataset is from kaggle official forum : https://www.kaggle.com/general/74235\n",
        "from google.colab import files\n",
        "print(\"Please upload your kaggle.jasn token file, to allow direct file download from kaggle to Colab\")\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c '2021s1comp5329assignment2'\n",
        "! unzip -q 2021s1comp5329assignment2.zip\n",
        "! rm 2021s1comp5329assignment2.zip\n",
        "\n",
        "data_root_dir = \"/content/COMP5329S1A2Dataset/data/\"\n",
        "print(\"DONE! image files in : \",data_root_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZH1TndQ5PG5"
      },
      "source": [
        "print(\"Please upload fixed train.csv and test.csv\")\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "train_csv=pd.read_csv(\"train.csv\")\n",
        "label_column=train_csv['Labels']\n",
        "img_column=train_csv['ImageID']\n",
        "print(label_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QHkkStK6ZsE"
      },
      "source": [
        "# create colums for each class\n",
        "col_list=np.arange(20)\n",
        "print(col_list)\n",
        "\n",
        "# create zeros in np with number of sample from train.csv\n",
        "labels=pd.DataFrame(np.zeros((len(train_csv),20), dtype=np.int), columns=col_list)\n",
        "print(\"zeros shape :\",labels.shape)\n",
        "\n",
        "# change lables matrix value for all samples with split\n",
        "index=0\n",
        "for row in label_column:\n",
        "    label_list=list(map(int, row.split(\" \")))\n",
        "    for i in label_list:\n",
        "        labels.iloc[index,i] = 1\n",
        "    index+=1\n",
        "print(labels)\n",
        "\n",
        "# combine image file name and label matrix\n",
        "# and store this dataframe to file\n",
        "processed_train_set = pd.concat([img_column, labels], axis=1)\n",
        "print(processed_train_set)\n",
        "processed_train_set.to_csv(\"processed_train.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHGBx4UUAR2k"
      },
      "source": [
        "# class distribution\n",
        "class_sum = labels.sum(axis=0)\n",
        "class_percentage = class_sum/class_sum.sum(axis=0)\n",
        "for it in col_list:\n",
        "    print(\"%3d\" %it,end='\\t')\n",
        "print()\n",
        "for it in col_list:\n",
        "    print(\"%.3f\" %class_percentage[it],end='\\t')\n",
        "\n",
        "# plot class distribution\n",
        "fig = plt.figure(figsize =(10, 6))\n",
        "plt.bar(col_list, class_percentage)\n",
        "plt.xticks(col_list, col_list)\n",
        "plt.show()\n",
        "\n",
        "# image dataset statistics\n",
        "image_infos = []\n",
        "tran_stat = transforms.Compose([transforms.ToTensor()])\n",
        "for it in processed_train_set.ImageID:\n",
        "    image = np.array(cv2.cvtColor(cv2.imread(data_root_dir+it), cv2.COLOR_BGR2RGB))\n",
        "    curr_infos = []\n",
        "    # image N rows and N cols\n",
        "    curr_infos.append(image.shape[0])\n",
        "    curr_infos.append(image.shape[1])\n",
        "    image = tran_stat(image)\n",
        "    # image RGB value means\n",
        "    curr_infos.append(image[:,:,0].mean())\n",
        "    curr_infos.append(image[:,:,1].mean())\n",
        "    curr_infos.append(image[:,:,2].mean())\n",
        "    # image RGB value stds\n",
        "    curr_infos.append(np.sqrt(image[:,:,0].var()+1e-5))\n",
        "    curr_infos.append(np.sqrt(image[:,:,1].var()+1e-5))\n",
        "    curr_infos.append(np.sqrt(image[:,:,2].var()+1e-5))\n",
        "    image_infos.append(curr_infos)\n",
        "\n",
        "image_infos = np.array(image_infos)\n",
        "print(\"\\t row \\t col \\t RGB means \\t RGB stds\")\n",
        "image_infos = image_infos.mean(axis=0)\n",
        "print(image_infos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjS6sdswO-8y"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_infos, data_root_dir=data_root_dir, csv_name=\"processed_train.csv\"):\n",
        "        self.image_infos = image_infos\n",
        "        print(self.image_infos)\n",
        "        self.data_root_dir = data_root_dir\n",
        "        self.csv_file_label = pd.read_csv(csv_name)\n",
        "        self.transforms = transforms.Compose([transforms.Resize((int(self.image_infos[0]),int(self.image_infos[1]))) , \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([self.image_infos[2], self.image_infos[3], self.image_infos[4]], \n",
        "                                                                 [self.image_infos[5], self.image_infos[6], self.image_infos[7]])])\n",
        "        #self.transforms = transforms.Compose([transforms.Resize((244,244)) , \n",
        "        #                       transforms.ToTensor(),\n",
        "        #                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        #                       ])\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file_label)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #image = cv2.cvtColor(cv2.imread(self.data_root_dir + self.csv_file_label.iloc[index].ImageID), cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(self.data_root_dir + self.csv_file_label.iloc[index].ImageID).convert(\"RGB\")\n",
        "        label = torch.tensor(self.csv_file_label.iloc[index][1:].tolist(), dtype=torch.float32)\n",
        "        \n",
        "        return self.transforms(image), label\n",
        "\n",
        "class ImageDataloader():\n",
        "    def __init__(self, image_infos, data_root_dir):\n",
        "        train_percentage = 0.85\n",
        "        image_dataset = ImageDataset(image_infos, data_root_dir, \"processed_train.csv\")\n",
        "        dataset_len = len(image_dataset)\n",
        "        self.train_set, self.valid_set = random_split(image_dataset, [int(dataset_len*train_percentage), (dataset_len-int(dataset_len*train_percentage))])\n",
        "\n",
        "    def make_loader(self, batch_size):\n",
        "        return DataLoader(self.train_set, shuffle=True, batch_size=batch_size), DataLoader(self.valid_set, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTKmwC3RFcbH"
      },
      "source": [
        "import torch.nn as nn\n",
        "class AsymmetricLoss(nn.Module):\n",
        "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
        "        super(AsymmetricLoss, self).__init__()\n",
        "\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.clip = clip\n",
        "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: input logits\n",
        "        y: targets (multi-label binarized vector)\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculating Probabilities\n",
        "        x_sigmoid = torch.sigmoid(x)\n",
        "        xs_pos = x_sigmoid\n",
        "        xs_neg = 1 - x_sigmoid\n",
        "\n",
        "        # Asymmetric Clipping\n",
        "        if self.clip is not None and self.clip > 0:\n",
        "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
        "\n",
        "        # Basic CE calculation\n",
        "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
        "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
        "        loss = los_pos + los_neg\n",
        "\n",
        "        # Asymmetric Focusing\n",
        "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(False)\n",
        "            pt0 = xs_pos * y\n",
        "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
        "            pt = pt0 + pt1\n",
        "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
        "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(True)\n",
        "            loss *= one_sided_w\n",
        "\n",
        "        return -loss.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdkFr2rTO0Q2"
      },
      "source": [
        "class resnet_50:\n",
        "    def __init__(self, N_classes=20, dropout_p=0.5, N_fc_layers=3):\n",
        "        \"\"\"\n",
        "        :type dropout_p: float\n",
        "        :param: dropout_p: the chance of nodes stay in the network\n",
        "        \"\"\"\n",
        "        #check device, if GPU avaliable for torch\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.N_classes = N_classes\n",
        "        # decide is need dropout, base on dropout_p value\n",
        "        # if dropout_p == 1, all node preserved, so no dropout\n",
        "        self.dropout_p = dropout_p\n",
        "        self.dropout = False\n",
        "        if self.dropout_p != 1.0:\n",
        "            self.dropout = True\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # connect Resnet50 to N_classes output fc\n",
        "        self.activ_func = nn.ReLU\n",
        "        N_fc_in = self.model.fc.in_features\n",
        "        self.N_fc_layers = N_fc_layers\n",
        "        fc_layers = []\n",
        "\n",
        "        if self.dropout:\n",
        "            for N_it in range(self.N_fc_layers):\n",
        "                fc_layers.append(nn.Linear(N_fc_in//(2**N_it), N_fc_in//(2**(N_it+1))))\n",
        "                fc_layers.append(nn.ReLU())\n",
        "                fc_layers.append(nn.BatchNorm1d(N_fc_in//(2**(N_it+1))))\n",
        "                # nn.dropout use probability of dropout as input\n",
        "                fc_layers.append(nn.Dropout(1-self.dropout_p))\n",
        "        else:\n",
        "            for N_it in range(self.N_fc_layers):\n",
        "                fc_layers.append(nn.Linear(N_fc_in//(2**N_it), N_fc_in//(2**(N_it+1))))\n",
        "                fc_layers.append(nn.ReLU())\n",
        "                fc_layers.append(nn.BatchNorm1d(N_fc_in//(2**(N_it+1))))\n",
        "        \n",
        "        # last layer to connect to N_classes\n",
        "        fc_layers.append(nn.Linear(N_fc_in//(2**(self.N_fc_layers)), self.N_classes))\n",
        "        # add custom fc layers to Resnet\n",
        "        self.model.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "        if DEBUG:\n",
        "            print(self.model)\n",
        "\n",
        "        # change device\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        #define the min threshold for prediction\n",
        "        self.predict_threshold = 0.5\n",
        "\n",
        "    def accuracy_calculate(self, y_true, outputs_torch, curr_batch_size):\n",
        "        y_true = y_true.to(torch.int).numpy()\n",
        "        y_pred = (torch.sigmoid(outputs_torch).data > self.predict_threshold).cpu().detach().to(torch.int).numpy()\n",
        "        return f1_score(y_true, y_pred, average='samples')*curr_batch_size\n",
        "\n",
        "    def train(self, train_dataloader, valid_dataloader, N_epochs=10, learning_rate=0.01, optimizer=\"adam\", scheduler=\"steplr\"):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.len_train = len(train_dataloader.dataset)\n",
        "        self.len_valid = len(valid_dataloader.dataset)\n",
        "\n",
        "        self.optimizer = None\n",
        "        # add training optimizers\n",
        "        if optimizer == \"adamw\":\n",
        "            print(\"optimizer : AdamW\")\n",
        "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "        elif optimizer == \"sparseadam\":\n",
        "            print(\"optimizer : SparseAdam\")\n",
        "            self.optimizer = torch.optim.SparseAdam(self.model.parameters(), lr=self.learning_rate)\n",
        "        else:\n",
        "            print(\"optimizer : Adam\")\n",
        "            # default as Adam\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # add training schedulers\n",
        "        self.scheduler = None\n",
        "        if scheduler == \"reducelr\":\n",
        "            print(\"scheduler : ReduceLROnPlateau\")\n",
        "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "        elif scheduler == \"cosinelr\":\n",
        "            print(\"scheduler : CosineAnnealingLR\")\n",
        "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=5, eta_min=0.005)\n",
        "        else:\n",
        "            print(\"scheduler : StepLR\")\n",
        "            # default as steplr\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # add training optimizaers\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        #self.criterion = AsymmetricLoss()\n",
        "\n",
        "        # recode evaluation matries for each epoch\n",
        "        #============================train==============================\n",
        "        for epoch_it in range(N_epochs):\n",
        "            #epoch loop\n",
        "            print(\"\\nepoch:\", epoch_it, '/', N_epochs,end=' ')\n",
        "            train_accuracy = 0.0 \n",
        "            train_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for images, labels in train_dataloader:\n",
        "                torch.set_grad_enabled(True)\n",
        "                # batch loop\n",
        "                images = images.to(self.device)\n",
        "                labels_processed = labels.to(self.device)\n",
        "                curr_batch_size = images.size(0)\n",
        "                \n",
        "                # get output and calculate accuracy\n",
        "                outputs = self.model(images)\n",
        "                train_accuracy += self.accuracy_calculate(labels, outputs, curr_batch_size)\n",
        "\n",
        "                # loss & backpropergate\n",
        "                loss = self.criterion(outputs, labels_processed)\n",
        "                loss.backward()\n",
        "                train_loss += curr_batch_size*loss.item()\n",
        "\n",
        "                # update parameters by grads & reset grads\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "            \n",
        "            # train print out\n",
        "            print(\"train loss:\", (train_loss/self.len_train),\n",
        "                  \"accuracy:\", (train_accuracy/self.len_train),end=' ')\n",
        "\n",
        "            #============================valid==============================\n",
        "            valid_accuracy = 0.0\n",
        "            valid_loss = 0.0\n",
        "            self.model.eval()\n",
        "       \n",
        "            for images, labels in valid_dataloader:\n",
        "                torch.set_grad_enabled(False)\n",
        "                # batch loop\n",
        "                images = images.to(self.device)\n",
        "                labels_processed = labels.to(self.device)\n",
        "                curr_batch_size = images.size(0)\n",
        "                \n",
        "                # get output and calculate accuracy\n",
        "                outputs = self.model(images)\n",
        "                valid_accuracy += self.accuracy_calculate(labels, outputs, curr_batch_size)\n",
        "\n",
        "                # loss & backpropergate\n",
        "                loss = self.criterion(outputs, labels_processed)\n",
        "                valid_loss += curr_batch_size*loss.item()\n",
        "\n",
        "            # valid print out\n",
        "            print(\"\\tvalid loss:\", (valid_loss/self.len_valid),\n",
        "                  \"accuracy\", (valid_accuracy/self.len_valid),end='')\n",
        "            \n",
        "            self.scheduler.step()   \n",
        "      \n",
        "      def save(name):\n",
        "        torch.save(model, \"my_model_\"+name+\".pth\")\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw0v6cm00PY1"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "for dropout_p in [0.8]:\n",
        "    print(\"\\n===============dropout_p:\",dropout_p)\n",
        "    i=1\n",
        "    for optimizer in [\"adam\", \"adamw\"]:\n",
        "        res50 = resnet_50(N_classes=20, dropout_p=dropout_p, N_fc_layers=3)\n",
        "\n",
        "        dataset_loader = ImageDataloader(image_infos, data_root_dir)\n",
        "        (train_set, valid_set) = dataset_loader.make_loader(batch_size = 128)\n",
        "        res50.train(train_dataloader=train_set, valid_dataloader=valid_set, N_epochs=20, learning_rate=0.001, optimizer=optimizer, scheduler=\"cosinelr\")\n",
        "        res50.save(i)\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsK4GBLI5Kla"
      },
      "source": [
        "def load_model(file):\n",
        "  return pytorch.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}