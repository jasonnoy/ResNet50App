{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5329assn2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgIGTgQ8BK7lAFy3o1RXrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonnoy/COMP5329NEW/blob/ver2_0/5329assn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P22itkey3yuS",
        "outputId": "936af3be-b585-4333-ccdb-0485ffd9004c"
      },
      "source": [
        "# mount google drive to colabif not mounted before\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbqypOO10hvP"
      },
      "source": [
        "# import necessary library\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "\n",
        "DEBUG = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "61DjxixN3QLF",
        "outputId": "9e285b7d-b9e3-4fa4-bb83-94a0e4259989"
      },
      "source": [
        "# download data from kaggle server with token file\n",
        "# must update before use in Colab!\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "# following method of download dataset is from kaggle official forum : https://www.kaggle.com/general/74235\n",
        "from google.colab import files\n",
        "print(\"Please upload your kaggle.jasn token file, to allow direct file download from kaggle to Colab\")\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c '2021s1comp5329assignment2'\n",
        "! unzip -q 2021s1comp5329assignment2.zip\n",
        "! rm 2021s1comp5329assignment2.zip\n",
        "\n",
        "data_root_dir = \"/content/COMP5329S1A2Dataset/data/\"\n",
        "print(\"DONE! image files in : \",data_root_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=01229d17f8461dfe8d176b3fbe460a18364206e63388144e2ee17853f5b523fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n",
            "Please upload your kaggle.jasn token file, to allow direct file download from kaggle to Colab\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f2c8fd73-9c3c-4789-af29-bec9b496c680\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f2c8fd73-9c3c-4789-af29-bec9b496c680\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading 2021s1comp5329assignment2.zip to /content\n",
            " 98% 1.29G/1.31G [00:10<00:00, 131MB/s]\n",
            "100% 1.31G/1.31G [00:10<00:00, 138MB/s]\n",
            "DONE! image files in :  /content/COMP5329S1A2Dataset/data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "BZH1TndQ5PG5",
        "outputId": "1f336224-8f4a-40a0-9410-ffc43a961f17"
      },
      "source": [
        "print(\"Please upload fixed train.csv and test.csv\")\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "train_csv=pd.read_csv(\"train.csv\")\n",
        "label_column=train_csv['Labels']\n",
        "img_column=train_csv['ImageID']\n",
        "print(label_column)\n",
        "\n",
        "test_csv=pd.read_csv(\"test.csv\")\n",
        "test_column=test_csv['ImageID']\n",
        "print(test_column)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please upload fixed train.csv and test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bbbc2b4c-b8a1-4a9b-b738-86c76e89c7a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bbbc2b4c-b8a1-4a9b-b738-86c76e89c7a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n",
            "0             1\n",
            "1          1 19\n",
            "2             1\n",
            "3        8 3 13\n",
            "4         8 3 7\n",
            "          ...  \n",
            "29995     8 1 2\n",
            "29996         1\n",
            "29997         1\n",
            "29998         1\n",
            "29999         1\n",
            "Name: Labels, Length: 30000, dtype: object\n",
            "0       30000.jpg\n",
            "1       30001.jpg\n",
            "2       30002.jpg\n",
            "3       30003.jpg\n",
            "4       30004.jpg\n",
            "          ...    \n",
            "9995    39995.jpg\n",
            "9996    39996.jpg\n",
            "9997    39997.jpg\n",
            "9998    39998.jpg\n",
            "9999    39999.jpg\n",
            "Name: ImageID, Length: 10000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QHkkStK6ZsE",
        "outputId": "11bc04b6-5997-4d58-9fb4-b812a7f098e3"
      },
      "source": [
        "# create colums for each class\n",
        "col_list=np.arange(20)\n",
        "print(col_list)\n",
        "\n",
        "# create zeros in np with number of sample from train.csv\n",
        "labels=pd.DataFrame(np.zeros((len(train_csv),20), dtype=np.int), columns=col_list)\n",
        "print(\"zeros shape :\",labels.shape)\n",
        "\n",
        "# change lables matrix value for all samples with split\n",
        "index=0\n",
        "for row in label_column:\n",
        "    label_list=list(map(int, row.split(\" \")))\n",
        "    for i in label_list:\n",
        "        labels.iloc[index,i] = 1\n",
        "    index+=1\n",
        "print(labels)\n",
        "\n",
        "# combine image file name and label matrix\n",
        "# and store this dataframe to file\n",
        "processed_train_set = pd.concat([img_column, labels], axis=1)\n",
        "print(processed_train_set)\n",
        "processed_train_set.to_csv(\"processed_train.csv\",index=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "zeros shape : (30000, 20)\n",
            "       0   1   2   3   4   5   6   7   8   ...  11  12  13  14  15  16  17  18  19\n",
            "0       0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "1       0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   1\n",
            "2       0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "3       0   0   0   1   0   0   0   0   1  ...   0   0   1   0   0   0   0   0   0\n",
            "4       0   0   0   1   0   0   0   1   1  ...   0   0   0   0   0   0   0   0   0\n",
            "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "29995   0   1   1   0   0   0   0   0   1  ...   0   0   0   0   0   0   0   0   0\n",
            "29996   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "29997   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "29998   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "29999   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "\n",
            "[30000 rows x 20 columns]\n",
            "         ImageID  0  1  2  3  4  5  6  7  ...  11  12  13  14  15  16  17  18  19\n",
            "0          0.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "1          1.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   1\n",
            "2          2.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "3          3.jpg  0  0  0  1  0  0  0  0  ...   0   0   1   0   0   0   0   0   0\n",
            "4          4.jpg  0  0  0  1  0  0  0  1  ...   0   0   0   0   0   0   0   0   0\n",
            "...          ... .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "29995  29995.jpg  0  1  1  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29996  29996.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29997  29997.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29998  29998.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29999  29999.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "\n",
            "[30000 rows x 21 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ZHGBx4UUAR2k",
        "outputId": "2f17c449-4851-402c-9b1a-db2bb02766ed"
      },
      "source": [
        "# class distribution\n",
        "class_sum = labels.sum(axis=0)\n",
        "class_percentage = class_sum/class_sum.sum(axis=0)\n",
        "for it in col_list:\n",
        "    print(\"%3d\" %it,end='\\t')\n",
        "print()\n",
        "for it in col_list:\n",
        "    print(\"%.3f\" %class_percentage[it],end='\\t')\n",
        "\n",
        "# plot class distribution\n",
        "fig = plt.figure(figsize =(10, 6))\n",
        "plt.bar(col_list, class_percentage)\n",
        "plt.xticks(col_list, col_list)\n",
        "plt.show()\n",
        "\n",
        "# image dataset statistics\n",
        "image_infos = []\n",
        "tran_stat = transforms.Compose([transforms.ToTensor()])\n",
        "for it in processed_train_set.ImageID:\n",
        "    image = np.array(cv2.cvtColor(cv2.imread(data_root_dir+it), cv2.COLOR_BGR2RGB))\n",
        "    curr_infos = []\n",
        "    # image N rows and N cols\n",
        "    curr_infos.append(image.shape[0])\n",
        "    curr_infos.append(image.shape[1])\n",
        "    image = tran_stat(image)\n",
        "    # image RGB value means\n",
        "    curr_infos.append(image[:,:,0].mean())\n",
        "    curr_infos.append(image[:,:,1].mean())\n",
        "    curr_infos.append(image[:,:,2].mean())\n",
        "    # image RGB value stds\n",
        "    curr_infos.append(np.sqrt(image[:,:,0].var()+1e-5))\n",
        "    curr_infos.append(np.sqrt(image[:,:,1].var()+1e-5))\n",
        "    curr_infos.append(np.sqrt(image[:,:,2].var()+1e-5))\n",
        "    image_infos.append(curr_infos)\n",
        "\n",
        "image_infos = np.array(image_infos)\n",
        "print(\"\\t row \\t col \\t RGB means \\t RGB stds\")\n",
        "image_infos = image_infos.mean(axis=0)\n",
        "print(image_infos)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0\t  1\t  2\t  3\t  4\t  5\t  6\t  7\t  8\t  9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t\n",
            "0.000\t0.490\t0.025\t0.094\t0.027\t0.024\t0.030\t0.026\t0.047\t0.022\t0.032\t0.013\t0.000\t0.013\t0.005\t0.042\t0.024\t0.031\t0.033\t0.022\t"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\t row \t col \t RGB means \t RGB stds\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjS6sdswO-8y"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_infos, data_root_dir=data_root_dir, csv_name=\"processed_train.csv\"):\n",
        "        self.image_infos = image_infos\n",
        "        print(self.image_infos)\n",
        "        self.data_root_dir = data_root_dir\n",
        "        self.csv_file_label = pd.read_csv(csv_name)\n",
        "        self.transforms = transforms.Compose([transforms.Resize((int(self.image_infos[0]),int(self.image_infos[1]))) , \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([self.image_infos[2], self.image_infos[3], self.image_infos[4]], \n",
        "                                                                 [self.image_infos[5], self.image_infos[6], self.image_infos[7]])])\n",
        "        #self.transforms = transforms.Compose([transforms.Resize((244,244)) , \n",
        "        #                       transforms.ToTensor(),\n",
        "        #                       transforms.Normalize([0.4, 0.4, 0.4], [0.2, 0.2, 0.2])\n",
        "        #                       ])\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file_label)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #image = cv2.cvtColor(cv2.imread(self.data_root_dir + self.csv_file_label.iloc[index].ImageID), cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(self.data_root_dir + self.csv_file_label.iloc[index].ImageID).convert(\"RGB\")\n",
        "        label = torch.tensor(self.csv_file_label.iloc[index][1:].tolist(), dtype=torch.float32)\n",
        "        \n",
        "        return self.transforms(image), label\n",
        "\n",
        "class ImageDataloader():\n",
        "    def __init__(self, image_infos, data_root_dir):\n",
        "        train_percentage = 0.85\n",
        "        image_dataset = ImageDataset(image_infos, data_root_dir, \"processed_train.csv\")\n",
        "        dataset_len = len(image_dataset)\n",
        "        self.train_set, self.valid_set = random_split(image_dataset, [int(dataset_len*train_percentage), (dataset_len-int(dataset_len*train_percentage))])\n",
        "\n",
        "    def make_loader(self, batch_size):\n",
        "        return DataLoader(self.train_set, shuffle=True, batch_size=batch_size), DataLoader(self.valid_set, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIkWkSOkJfQ"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_infos, data_root_dir=data_root_dir, csv_name=\"test.csv\"):\n",
        "        self.image_infos = image_infos\n",
        "        print(self.image_infos)\n",
        "        self.data_root_dir = data_root_dir\n",
        "        self.csv_file_label = pd.read_csv(csv_name)\n",
        "        self.transforms = transforms.Compose([transforms.Resize((int(self.image_infos[0]),int(self.image_infos[1]))) , \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([self.image_infos[2], self.image_infos[3], self.image_infos[4]], \n",
        "                                                                 [self.image_infos[5], self.image_infos[6], self.image_infos[7]])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file_label)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #image = cv2.cvtColor(cv2.imread(self.data_root_dir + self.csv_file_label.iloc[index].ImageID), cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(self.data_root_dir + self.csv_file_label.iloc[index].ImageID).convert(\"RGB\")\n",
        "        image_ID = self.csv_file_label.iloc[index].ImageID\n",
        "        \n",
        "        return self.transforms(image), image_ID\n",
        "\n",
        "class TestDataloader():\n",
        "    def __init__(self, image_infos, data_root_dir):\n",
        "        self.test_set = TestDataset(image_infos, data_root_dir, \"test.csv\")\n",
        "\n",
        "    def make_loader(self, batch_size):\n",
        "        return DataLoader(self.test_set, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTKmwC3RFcbH"
      },
      "source": [
        "import torch.nn as nn\n",
        "class AsymmetricLoss(nn.Module):\n",
        "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
        "        super(AsymmetricLoss, self).__init__()\n",
        "\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.clip = clip\n",
        "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: input logits\n",
        "        y: targets (multi-label binarized vector)\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculating Probabilities\n",
        "        x_sigmoid = torch.sigmoid(x)\n",
        "        xs_pos = x_sigmoid\n",
        "        xs_neg = 1 - x_sigmoid\n",
        "\n",
        "        # Asymmetric Clipping\n",
        "        if self.clip is not None and self.clip > 0:\n",
        "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
        "\n",
        "        # Basic CE calculation\n",
        "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
        "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
        "        loss = los_pos + los_neg\n",
        "\n",
        "        # Asymmetric Focusing\n",
        "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(False)\n",
        "            pt0 = xs_pos * y\n",
        "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
        "            pt = pt0 + pt1\n",
        "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
        "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(True)\n",
        "            loss *= one_sided_w\n",
        "\n",
        "        return -loss.sum()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdkFr2rTO0Q2"
      },
      "source": [
        "class resnet_50:\n",
        "    def __init__(self, N_classes=20, dropout_p=0.5, N_fc_layers=3, n_reduce=4):\n",
        "        \"\"\"\n",
        "        :type dropout_p: float\n",
        "        :param: dropout_p: the chance of nodes stay in the network\n",
        "        \"\"\"\n",
        "        #check device, if GPU avaliable for torch\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "\n",
        "        #self.model = models.resnet50(pretrained=True)\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.N_classes = N_classes\n",
        "        # decide is need dropout, base on dropout_p value\n",
        "        # if dropout_p == 1, all node preserved, so no dropout\n",
        "        self.dropout_p = dropout_p\n",
        "        self.dropout = False\n",
        "        if self.dropout_p != 1.0:\n",
        "            self.dropout = True\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # connect Resnet50 to N_classes output fc\n",
        "        self.activ_func = nn.ReLU\n",
        "        N_fc_in = self.model.fc.in_features\n",
        "        self.N_fc_layers = N_fc_layers\n",
        "        fc_layers = []\n",
        "\n",
        "        n_reduce = 4\n",
        "\n",
        "        if self.dropout:\n",
        "            for N_it in range(self.N_fc_layers):\n",
        "                fc_layers.append(nn.Linear(N_fc_in//(n_reduce**N_it), N_fc_in//(n_reduce**(N_it+1))))\n",
        "                fc_layers.append(nn.ReLU())\n",
        "                fc_layers.append(nn.BatchNorm1d(N_fc_in//(n_reduce**(N_it+1))))\n",
        "                # nn.dropout use probability of dropout as input\n",
        "                fc_layers.append(nn.Dropout(1-self.dropout_p))\n",
        "        else:\n",
        "            for N_it in range(self.N_fc_layers):\n",
        "                fc_layers.append(nn.Linear(N_fc_in//(n_reduce**N_it), N_fc_in//(n_reduce**(N_it+1))))\n",
        "                fc_layers.append(nn.ReLU())\n",
        "                fc_layers.append(nn.BatchNorm1d(N_fc_in//(n_reduce**(N_it+1))))\n",
        "        \n",
        "        # last layer to connect to N_classes\n",
        "        fc_layers.append(nn.Linear(N_fc_in//(n_reduce**(self.N_fc_layers)), self.N_classes))\n",
        "        # add custom fc layers to Resnet\n",
        "        self.model.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "        if DEBUG:\n",
        "            print(self.model)\n",
        "\n",
        "        # change device\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        #define the min threshold for prediction\n",
        "        self.predict_threshold = 0.5\n",
        "\n",
        "    def accuracy_calculate(self, y_true, outputs_torch, curr_batch_size):\n",
        "        y_true = y_true.to(torch.int).numpy()\n",
        "        y_pred = (torch.sigmoid(outputs_torch).data > self.predict_threshold).cpu().detach().to(torch.int).numpy()\n",
        "        return f1_score(y_true, y_pred, average='samples')*curr_batch_size\n",
        "\n",
        "    def train(self, train_dataloader, valid_dataloader, N_epochs=10, learning_rate=0.01, optimizer=\"adam\", scheduler=\"steplr\"):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.len_train = len(train_dataloader.dataset)\n",
        "        self.len_valid = len(valid_dataloader.dataset)\n",
        "\n",
        "        self.optimizer = None\n",
        "        # add training optimizers\n",
        "        if optimizer == \"adamw\":\n",
        "            print(\"optimizer : AdamW\")\n",
        "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "        elif optimizer == \"sparseadam\":\n",
        "            print(\"optimizer : SparseAdam\")\n",
        "            self.optimizer = torch.optim.SparseAdam(self.model.parameters(), lr=self.learning_rate)\n",
        "        else:\n",
        "            print(\"optimizer : Adam\")\n",
        "            # default as Adam\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # add training schedulers\n",
        "        self.scheduler = None\n",
        "        if scheduler == \"reducelr\":\n",
        "            print(\"scheduler : ReduceLROnPlateau\")\n",
        "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "        elif scheduler == \"cosinelr\":\n",
        "            print(\"scheduler : CosineAnnealingLR\")\n",
        "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=5, eta_min=0.005)\n",
        "        else:\n",
        "            print(\"scheduler : StepLR\")\n",
        "            # default as steplr\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # add training optimizaers\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        #self.criterion = AsymmetricLoss()\n",
        "\n",
        "        # recode evaluation matries for each epoch\n",
        "        #============================train==============================\n",
        "        for epoch_it in range(N_epochs):\n",
        "            #epoch loop\n",
        "            print(\"epoch:{}/{}\".format((epoch_it+1),N_epochs),end=' ')\n",
        "            train_accuracy = 0.0 \n",
        "            train_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for images, labels in train_dataloader:\n",
        "                torch.set_grad_enabled(True)\n",
        "                # batch loop\n",
        "                images = images.to(self.device)\n",
        "                labels_processed = labels.to(self.device)\n",
        "                curr_batch_size = images.size(0)\n",
        "                \n",
        "                # get output and calculate accuracy\n",
        "                outputs = self.model(images)\n",
        "                train_accuracy += self.accuracy_calculate(labels, outputs, curr_batch_size)\n",
        "\n",
        "                # loss & backpropergate\n",
        "                loss = self.criterion(outputs, labels_processed)\n",
        "                loss.backward()\n",
        "                train_loss += curr_batch_size*loss.item()\n",
        "\n",
        "                # update parameters by grads & reset grads\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "            \n",
        "            # train print out\n",
        "            print(\"train loss:\", (train_loss/self.len_train),\n",
        "                  \"accuracy:\", (train_accuracy/self.len_train),end=' ')\n",
        "\n",
        "            #============================valid==============================\n",
        "            valid_accuracy = 0.0\n",
        "            valid_loss = 0.0\n",
        "            self.model.eval()\n",
        "       \n",
        "            for images, labels in valid_dataloader:\n",
        "                torch.set_grad_enabled(False)\n",
        "                # batch loop\n",
        "                images = images.to(self.device)\n",
        "                labels_processed = labels.to(self.device)\n",
        "                curr_batch_size = images.size(0)\n",
        "                \n",
        "                # get output and calculate accuracy\n",
        "                outputs = self.model(images)\n",
        "                valid_accuracy += self.accuracy_calculate(labels, outputs, curr_batch_size)\n",
        "\n",
        "                # loss & backpropergate\n",
        "                loss = self.criterion(outputs, labels_processed)\n",
        "                valid_loss += curr_batch_size*loss.item()\n",
        "\n",
        "            # valid print out\n",
        "            print(\"\\tvalid loss:\", (valid_loss/self.len_valid),\n",
        "                  \"accuracy\", (valid_accuracy/self.len_valid))\n",
        "            \n",
        "            self.scheduler.step()\n",
        "\n",
        "    def test_pred(self, outputs_torch, curr_batch_size):\n",
        "        y_pred = (torch.sigmoid(outputs_torch).data > self.predict_threshold).cpu().detach().to(torch.int).numpy()\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, test_dataloader, output_file_name='Predicted_labels.txt'):\n",
        "    #============================test==============================\n",
        "        self.model.eval()\n",
        "        output_file = open(output_file_name,\"w+\")\n",
        "\n",
        "        for images, image_ID in test_dataloader:\n",
        "            torch.set_grad_enabled(False)\n",
        "            # batch loop\n",
        "            images = images.to(self.device)\n",
        "            curr_batch_size = images.size(0)\n",
        "            \n",
        "            # get output and calculate accuracy\n",
        "            outputs = self.model(images)\n",
        "\n",
        "            y_preds = self.test_pred(outputs, curr_batch_size)\n",
        "            output_file.write(\"ImageID,Labels\")\n",
        "\n",
        "            for it in range(len(y_preds)):\n",
        "                output_file.write(\"%s,\"%image_ID[it])\n",
        "\n",
        "                pred_label = np.atleast_1d(np.squeeze(np.argwhere(y_preds[it]==1)))\n",
        "                for label_it in pred_label:\n",
        "                    output_file.write(\" %d\"%label_it)\n",
        "                output_file.write('\\n')\n",
        "\n",
        "        output_file.close()        \n",
        "\n",
        "    def save(self, name):\n",
        "        torch.save(self.model, \"my_model_\"+name+\".pth\")\n",
        "\n",
        "    def load(self, file):\n",
        "        self.model = torch.load(file)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw0v6cm00PY1"
      },
      "source": [
        "#DEBUG = False\n",
        "\n",
        "#for dropout_p in [0.8]:\n",
        "#    print(\"\\n===============dropout_p:\",dropout_p)\n",
        "#    for optimizer in [\"adam\", \"adamw\"]:\n",
        "#        res50 = resnet_50(N_classes=20, dropout_p=dropout_p, N_fc_layers=3)\n",
        "\n",
        "#        dataset_loader = ImageDataloader(image_infos, data_root_dir)\n",
        "#        (train_set, valid_set) = dataset_loader.make_loader(batch_size = 128)\n",
        "#        res50.train(train_dataloader=train_set, valid_dataloader=valid_set, N_epochs=20, learning_rate=0.001, optimizer=optimizer, scheduler=\"cosinelr\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmkgADj4lr3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2483fcd8-4684-4868-834e-183d8f35ed36"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "for n_reduce in [2, 4, 8, 16]:\n",
        "    for N_fc_layers in [0,1]:\n",
        "        print(\"n_reduce:\",n_reduce)\n",
        "        print(\"N_fc_layers\",N_fc_layers)\n",
        "        res50 = resnet_50(N_classes=20, dropout_p=0.8, N_fc_layers=N_fc_layers, n_reduce=n_reduce)\n",
        "\n",
        "        dataset_loader = ImageDataloader(image_infos, data_root_dir)\n",
        "        (train_set, valid_set) = dataset_loader.make_loader(batch_size = 256)\n",
        "        res50.train(train_dataloader=train_set, valid_dataloader=valid_set, N_epochs=20, learning_rate=0.001, optimizer=\"adam\", scheduler=\"cosinelr\")\n",
        "\n",
        "#test_set = TestDataloader(image_infos, data_root_dir).make_loader(batch_size = 128)\n",
        "#res50.predict(test_set)\n",
        "#files.download('Predicted_labels.txt')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_reduce: 2\n",
            "N_fc_layers 0\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.1604238142090685 accuracy: 0.6398300070300071 \tvalid loss: 0.11939481035868327 accuracy 0.6881024691358025\n",
            "epoch:2/20 train loss: 0.10542134037789176 accuracy: 0.7282150751209574 \tvalid loss: 0.10047249515189065 accuracy 0.7618846400513067\n",
            "epoch:3/20 train loss: 0.09413779908652399 accuracy: 0.7750598930481282 \tvalid loss: 0.09444518268770642 accuracy 0.7826477954144619\n",
            "epoch:4/20 train loss: 0.09000945569720922 accuracy: 0.7913967291967294 \tvalid loss: 0.09356018837955263 accuracy 0.7962497033830368\n",
            "epoch:5/20 train loss: 0.08855899834516001 accuracy: 0.7966537758537754 \tvalid loss: 0.09414300115903218 accuracy 0.7956830687830687\n",
            "epoch:6/20 train loss: 0.08743038860723085 accuracy: 0.8003805540864362 \tvalid loss: 0.09570597394969728 accuracy 0.8047226390893057\n",
            "epoch:7/20 train loss: 0.08530733226561078 accuracy: 0.8056590894943836 \tvalid loss: 0.0944766227139367 accuracy 0.7964832451499119\n",
            "epoch:8/20 train loss: 0.08311087708964067 accuracy: 0.8104943411708116 \tvalid loss: 0.08925735047128465 accuracy 0.804065512265512\n",
            "epoch:9/20 train loss: 0.08043348452156665 accuracy: 0.81410739043092 \tvalid loss: 0.09013382652070787 accuracy 0.8068923200256533\n",
            "epoch:10/20 train loss: 0.07867564548698126 accuracy: 0.8174888860594746 \tvalid loss: 0.0887614339126481 accuracy 0.811796376463043\n",
            "epoch:11/20 train loss: 0.07810026668567284 accuracy: 0.8197337407690349 \tvalid loss: 0.08726204846302668 accuracy 0.8114988455988456\n",
            "epoch:12/20 train loss: 0.07814255123278674 accuracy: 0.8182008856067676 \tvalid loss: 0.08851803416675992 accuracy 0.8095076639409974\n",
            "epoch:13/20 train loss: 0.07995287309090296 accuracy: 0.8165055399937752 \tvalid loss: 0.0880926189886199 accuracy 0.8184144620811287\n",
            "epoch:14/20 train loss: 0.08240557194574206 accuracy: 0.8118643352290412 \tvalid loss: 0.0914752330382665 accuracy 0.8133654481321149\n",
            "epoch:15/20 train loss: 0.08291704779045254 accuracy: 0.8142501161148219 \tvalid loss: 0.09468659204244613 accuracy 0.8101073753407088\n",
            "epoch:16/20 train loss: 0.08258928431482876 accuracy: 0.8133302888832296 \tvalid loss: 0.09132725481854545 accuracy 0.819460301426968\n",
            "epoch:17/20 train loss: 0.0797893883340499 accuracy: 0.8208140508728741 \tvalid loss: 0.09090351963043213 accuracy 0.8105460317460316\n",
            "epoch:18/20 train loss: 0.07786346705520854 accuracy: 0.8216194097841155 \tvalid loss: 0.08845500257280138 accuracy 0.8119369408369408\n",
            "epoch:19/20 train loss: 0.07587180439864888 accuracy: 0.8253123588829472 \tvalid loss: 0.08871645913521449 accuracy 0.8172892095558761\n",
            "epoch:20/20 train loss: 0.07424996326250188 accuracy: 0.8287099086099083 \tvalid loss: 0.08811620914273792 accuracy 0.816695494628828\n",
            "n_reduce: 2\n",
            "N_fc_layers 1\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.3919189905909931 accuracy: 0.5734429725275836 \tvalid loss: 0.15536522383160062 accuracy 0.7671748950396007\n",
            "epoch:2/20 train loss: 0.0990142733244335 accuracy: 0.7840247517188695 \tvalid loss: 0.09677764562765757 accuracy 0.7901232323232322\n",
            "epoch:3/20 train loss: 0.09026567270592148 accuracy: 0.7956785808962281 \tvalid loss: 0.09252557124694188 accuracy 0.8125926086259418\n",
            "epoch:4/20 train loss: 0.08745609092128043 accuracy: 0.7996921668745195 \tvalid loss: 0.0907621645198928 accuracy 0.7977286355619688\n",
            "epoch:5/20 train loss: 0.085195411706672 accuracy: 0.8065042865631099 \tvalid loss: 0.09084885829024844 accuracy 0.8039633637966971\n",
            "epoch:6/20 train loss: 0.08388735904062496 accuracy: 0.8103234501881557 \tvalid loss: 0.08979083685742484 accuracy 0.784399903799904\n",
            "epoch:7/20 train loss: 0.08131212932455774 accuracy: 0.8140320148261326 \tvalid loss: 0.09057422731320064 accuracy 0.8089179733846401\n",
            "epoch:8/20 train loss: 0.07881031084411284 accuracy: 0.819947242735478 \tvalid loss: 0.08670391719208824 accuracy 0.8134109026775692\n",
            "epoch:9/20 train loss: 0.07635507445767814 accuracy: 0.8263433551198258 \tvalid loss: 0.08610340854194429 accuracy 0.8127699855699855\n",
            "epoch:10/20 train loss: 0.07353712960201152 accuracy: 0.8333484056248766 \tvalid loss: 0.08525306915574604 accuracy 0.8160801507134842\n",
            "epoch:11/20 train loss: 0.07201283588362675 accuracy: 0.8366634043516398 \tvalid loss: 0.08476911432875528 accuracy 0.8193748597081931\n",
            "epoch:12/20 train loss: 0.07193256448530683 accuracy: 0.8366745409274826 \tvalid loss: 0.08732394455538856 accuracy 0.81259645662979\n",
            "epoch:13/20 train loss: 0.07394913159047856 accuracy: 0.8326157089098266 \tvalid loss: 0.08895496774382061 accuracy 0.817152701619368\n",
            "epoch:14/20 train loss: 0.076907861114717 accuracy: 0.8237492827434006 \tvalid loss: 0.08983497721619076 accuracy 0.8140847362514029\n",
            "epoch:15/20 train loss: 0.07873906130066105 accuracy: 0.8198600680365383 \tvalid loss: 0.0893737030227979 accuracy 0.8005444925444926\n",
            "epoch:16/20 train loss: 0.07795274848447127 accuracy: 0.8228637919814387 \tvalid loss: 0.09307400090826883 accuracy 0.8078230078563413\n",
            "epoch:17/20 train loss: 0.07650922869233524 accuracy: 0.8251578743696392 \tvalid loss: 0.08885056022802988 accuracy 0.8062444444444444\n",
            "epoch:18/20 train loss: 0.07445311062593087 accuracy: 0.8302894373600254 \tvalid loss: 0.09163867916663487 accuracy 0.7952414141414141\n",
            "epoch:19/20 train loss: 0.07141652371486028 accuracy: 0.8375819256995728 \tvalid loss: 0.08827913564443589 accuracy 0.8207094917428251\n",
            "epoch:20/20 train loss: 0.06822604038551743 accuracy: 0.8474041984159627 \tvalid loss: 0.08853362739748424 accuracy 0.817256405323072\n",
            "n_reduce: 4\n",
            "N_fc_layers 0\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.16381068600860296 accuracy: 0.6374196791610285 \tvalid loss: 0.11771645638677809 accuracy 0.6875509700176365\n",
            "epoch:2/20 train loss: 0.10655120741152296 accuracy: 0.7250432957021193 \tvalid loss: 0.0989491732650333 accuracy 0.7616985890652558\n",
            "epoch:3/20 train loss: 0.09503280735833972 accuracy: 0.7719546444840566 \tvalid loss: 0.09216951573557323 accuracy 0.7910633156966489\n",
            "epoch:4/20 train loss: 0.09025523178834541 accuracy: 0.7894118863707102 \tvalid loss: 0.09180234578582976 accuracy 0.8017751322751321\n",
            "epoch:5/20 train loss: 0.0895494335434016 accuracy: 0.7948657754010694 \tvalid loss: 0.0933849611348576 accuracy 0.8077329805996473\n",
            "epoch:6/20 train loss: 0.08756200984763164 accuracy: 0.8011499108734405 \tvalid loss: 0.09298719294203653 accuracy 0.7741048260381594\n",
            "epoch:7/20 train loss: 0.08590001356367971 accuracy: 0.8055160965396265 \tvalid loss: 0.08974394616153505 accuracy 0.8147475228475229\n",
            "epoch:8/20 train loss: 0.08401438126377031 accuracy: 0.8096304727951786 \tvalid loss: 0.08797211784124374 accuracy 0.808595414462081\n",
            "epoch:9/20 train loss: 0.0806861456749486 accuracy: 0.8139168887757122 \tvalid loss: 0.08655176818370819 accuracy 0.8155191277857944\n",
            "epoch:10/20 train loss: 0.07896279594126869 accuracy: 0.8172367399485047 \tvalid loss: 0.08572085293134053 accuracy 0.8083730960397627\n",
            "epoch:11/20 train loss: 0.07821523279185388 accuracy: 0.8191451008686305 \tvalid loss: 0.08553013563818401 accuracy 0.8068402918069586\n",
            "epoch:12/20 train loss: 0.07888643567117991 accuracy: 0.8189420932009166 \tvalid loss: 0.08528769571251339 accuracy 0.8116499919833251\n",
            "epoch:13/20 train loss: 0.08048918816271951 accuracy: 0.8155964858670737 \tvalid loss: 0.08741055997875001 accuracy 0.8164939073272406\n",
            "epoch:14/20 train loss: 0.08229742617116255 accuracy: 0.8143493748081981 \tvalid loss: 0.08820005736748378 accuracy 0.8132124098124096\n",
            "epoch:15/20 train loss: 0.08365274469875822 accuracy: 0.8124622414622412 \tvalid loss: 0.08924945924679438 accuracy 0.8065925284591952\n",
            "epoch:16/20 train loss: 0.0826352370802094 accuracy: 0.8142189287836343 \tvalid loss: 0.09228626408179601 accuracy 0.7859694404361071\n",
            "epoch:17/20 train loss: 0.08049247490892224 accuracy: 0.8189701851525379 \tvalid loss: 0.09315928046570884 accuracy 0.7942301587301586\n",
            "epoch:18/20 train loss: 0.07901550504740547 accuracy: 0.8214806326571036 \tvalid loss: 0.0874829975631502 accuracy 0.8089452781786115\n",
            "epoch:19/20 train loss: 0.07609152087043314 accuracy: 0.827352134793311 \tvalid loss: 0.08618210255437427 accuracy 0.8137060766394099\n",
            "epoch:20/20 train loss: 0.07440993673661175 accuracy: 0.8285525648643294 \tvalid loss: 0.08524211619297663 accuracy 0.8163007856341188\n",
            "n_reduce: 4\n",
            "N_fc_layers 1\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.38374343810829464 accuracy: 0.5795771904539068 \tvalid loss: 0.15047645848327212 accuracy 0.7875540607540606\n",
            "epoch:2/20 train loss: 0.09744242079351463 accuracy: 0.787000722588958 \tvalid loss: 0.09199658903148439 accuracy 0.7860146384479717\n",
            "epoch:3/20 train loss: 0.08971148410381055 accuracy: 0.7954544984427339 \tvalid loss: 0.08982791381412082 accuracy 0.8015428571428571\n",
            "epoch:4/20 train loss: 0.08787223628221774 accuracy: 0.8013378830320013 \tvalid loss: 0.09018864452176624 accuracy 0.7896820747154081\n",
            "epoch:5/20 train loss: 0.08563336396100474 accuracy: 0.8048484678719974 \tvalid loss: 0.0946424569355117 accuracy 0.7917803110469778\n",
            "epoch:6/20 train loss: 0.08429635998080759 accuracy: 0.8086316922728685 \tvalid loss: 0.08891414426432716 accuracy 0.8067189193522527\n",
            "epoch:7/20 train loss: 0.08211677302449358 accuracy: 0.8138913393795749 \tvalid loss: 0.08735125115182665 accuracy 0.7989444765111433\n",
            "epoch:8/20 train loss: 0.07997583253710877 accuracy: 0.8185655392537745 \tvalid loss: 0.08917673125531939 accuracy 0.793278098444765\n",
            "epoch:9/20 train loss: 0.07714666079773623 accuracy: 0.8253811589282174 \tvalid loss: 0.08694141303168403 accuracy 0.8108161776495109\n",
            "epoch:10/20 train loss: 0.07434524206787932 accuracy: 0.8315717086834733 \tvalid loss: 0.08479843553569581 accuracy 0.809734455667789\n",
            "epoch:11/20 train loss: 0.0723155086239179 accuracy: 0.8373609075020838 \tvalid loss: 0.08486398671733009 accuracy 0.8212794292127623\n",
            "epoch:12/20 train loss: 0.07236137137459774 accuracy: 0.8355443610855375 \tvalid loss: 0.08623516991403368 accuracy 0.8221796857463524\n",
            "epoch:13/20 train loss: 0.07455800332391964 accuracy: 0.8327283479048183 \tvalid loss: 0.08682586789131165 accuracy 0.8070150873817541\n",
            "epoch:14/20 train loss: 0.07758851313006644 accuracy: 0.8232937541761072 \tvalid loss: 0.08726482803954018 accuracy 0.8065311848645184\n",
            "epoch:15/20 train loss: 0.07909230685467813 accuracy: 0.8192690575219986 \tvalid loss: 0.09312653093867831 accuracy 0.7807273368606702\n",
            "epoch:16/20 train loss: 0.07893833309061388 accuracy: 0.8196121325297795 \tvalid loss: 0.09211896136734221 accuracy 0.8134199935866604\n",
            "epoch:17/20 train loss: 0.07759771308478187 accuracy: 0.8236594390359094 \tvalid loss: 0.09442116118801964 accuracy 0.8117595527928863\n",
            "epoch:18/20 train loss: 0.07476739369654188 accuracy: 0.8299549783549779 \tvalid loss: 0.08924638268020418 accuracy 0.8056987814654482\n",
            "epoch:19/20 train loss: 0.0717031965431045 accuracy: 0.8381596553773022 \tvalid loss: 0.09003040326303906 accuracy 0.81583949013949\n",
            "epoch:20/20 train loss: 0.06870646016854866 accuracy: 0.8458183940242765 \tvalid loss: 0.08703930097156101 accuracy 0.8137527336860669\n",
            "n_reduce: 8\n",
            "N_fc_layers 0\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.1639613437278598 accuracy: 0.6403215979842277 \tvalid loss: 0.12122241415580114 accuracy 0.6774904761904762\n",
            "epoch:2/20 train loss: 0.10666528136940563 accuracy: 0.7284052881758762 \tvalid loss: 0.10041811112562815 accuracy 0.7469117203783872\n",
            "epoch:3/20 train loss: 0.0955727840884059 accuracy: 0.7714577115694764 \tvalid loss: 0.09449482622411516 accuracy 0.7903323552990218\n",
            "epoch:4/20 train loss: 0.09189489160682641 accuracy: 0.7904957502192794 \tvalid loss: 0.09254427309830983 accuracy 0.783611367644701\n",
            "epoch:5/20 train loss: 0.0895170305464782 accuracy: 0.7989634581105173 \tvalid loss: 0.09238335769044029 accuracy 0.7706780984447652\n",
            "epoch:6/20 train loss: 0.08737080472006517 accuracy: 0.802070609564727 \tvalid loss: 0.09115152351061503 accuracy 0.7908787718454384\n",
            "epoch:7/20 train loss: 0.08619364696974847 accuracy: 0.8064633647398352 \tvalid loss: 0.09077404254012637 accuracy 0.8035808882475547\n",
            "epoch:8/20 train loss: 0.08331044934543909 accuracy: 0.811041448094389 \tvalid loss: 0.0882472545173433 accuracy 0.8048239538239539\n",
            "epoch:9/20 train loss: 0.0806409108545266 accuracy: 0.815883612030671 \tvalid loss: 0.08793441814184189 accuracy 0.7954632194965529\n",
            "epoch:10/20 train loss: 0.07914897915311889 accuracy: 0.8179687038451744 \tvalid loss: 0.08664476158883837 accuracy 0.8092312810646144\n",
            "epoch:11/20 train loss: 0.07850764689024757 accuracy: 0.819731127913481 \tvalid loss: 0.08631180508931478 accuracy 0.799275998075998\n",
            "epoch:12/20 train loss: 0.07905951748174779 accuracy: 0.8181195055489172 \tvalid loss: 0.08694323988755544 accuracy 0.806505643738977\n",
            "epoch:13/20 train loss: 0.0798522739819452 accuracy: 0.8170206477618241 \tvalid loss: 0.0895166301396158 accuracy 0.803071492704826\n",
            "epoch:14/20 train loss: 0.08129959435907065 accuracy: 0.8163923294570354 \tvalid loss: 0.09150587509738074 accuracy 0.7955184223184223\n",
            "epoch:15/20 train loss: 0.08262639126473782 accuracy: 0.8133618538324421 \tvalid loss: 0.09183210006687376 accuracy 0.8058345198011866\n",
            "epoch:16/20 train loss: 0.08368164971763012 accuracy: 0.8139198309315957 \tvalid loss: 0.09003203631109662 accuracy 0.7859042488375821\n",
            "epoch:17/20 train loss: 0.08141571152093363 accuracy: 0.8182621169679998 \tvalid loss: 0.08661827113231023 accuracy 0.811770931537598\n",
            "epoch:18/20 train loss: 0.07830029389437507 accuracy: 0.8224664601194016 \tvalid loss: 0.08657236229048836 accuracy 0.805218678852012\n",
            "epoch:19/20 train loss: 0.07633040478065903 accuracy: 0.826551407851408 \tvalid loss: 0.08838114888138242 accuracy 0.805279621612955\n",
            "epoch:20/20 train loss: 0.07466965485787859 accuracy: 0.8285343604108308 \tvalid loss: 0.0858043047785759 accuracy 0.8115181176847843\n",
            "n_reduce: 8\n",
            "N_fc_layers 1\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.3839485581026358 accuracy: 0.5842358061652086 \tvalid loss: 0.10865756761365467 accuracy 0.757774330607664\n",
            "epoch:2/20 train loss: 0.09700261796455757 accuracy: 0.7862929208046858 \tvalid loss: 0.09034702999724283 accuracy 0.7931318101651433\n",
            "epoch:3/20 train loss: 0.08883285367839476 accuracy: 0.7991905880611762 \tvalid loss: 0.09134924688604143 accuracy 0.792990075356742\n",
            "epoch:4/20 train loss: 0.08732903465803932 accuracy: 0.8011270296152646 \tvalid loss: 0.0892440220978525 accuracy 0.802058682058682\n",
            "epoch:5/20 train loss: 0.08533715875476014 accuracy: 0.8066388931330108 \tvalid loss: 0.08906343152125676 accuracy 0.8089775052108386\n",
            "epoch:6/20 train loss: 0.08331338449669819 accuracy: 0.8101369155898572 \tvalid loss: 0.09152876417504417 accuracy 0.8162072470739138\n",
            "epoch:7/20 train loss: 0.08137725565947738 accuracy: 0.8148191551368026 \tvalid loss: 0.09071603170368407 accuracy 0.7812736251402917\n",
            "epoch:8/20 train loss: 0.07877447590290332 accuracy: 0.8224502673796787 \tvalid loss: 0.0892578596274058 accuracy 0.8158306076639409\n",
            "epoch:9/20 train loss: 0.07597264611954782 accuracy: 0.827745822369352 \tvalid loss: 0.10284864241547055 accuracy 0.7829664101330768\n",
            "epoch:10/20 train loss: 0.07337233031497283 accuracy: 0.8344515491044902 \tvalid loss: 0.0827116290198432 accuracy 0.8206937950937951\n",
            "epoch:11/20 train loss: 0.07127265462454628 accuracy: 0.8389279631044337 \tvalid loss: 0.08332017074028651 accuracy 0.82604329004329\n",
            "epoch:12/20 train loss: 0.07116348922837014 accuracy: 0.83996357694593 \tvalid loss: 0.08681397130754259 accuracy 0.8089625140291807\n",
            "epoch:13/20 train loss: 0.07373343889502917 accuracy: 0.8324529496647144 \tvalid loss: 0.08665112307336595 accuracy 0.8059233124899792\n",
            "epoch:14/20 train loss: 0.07698764895925335 accuracy: 0.8267867019690548 \tvalid loss: 0.08912894816531075 accuracy 0.8121428411095076\n",
            "epoch:15/20 train loss: 0.07779935256523245 accuracy: 0.8238062105650342 \tvalid loss: 0.09032777203453912 accuracy 0.8103096360429695\n",
            "epoch:16/20 train loss: 0.07874437461062973 accuracy: 0.8202548934725405 \tvalid loss: 0.08804736384418275 accuracy 0.8093244989578324\n",
            "epoch:17/20 train loss: 0.07657650330019933 accuracy: 0.8239043601278897 \tvalid loss: 0.08805810120370652 accuracy 0.8177317780984447\n",
            "epoch:18/20 train loss: 0.07410049475291196 accuracy: 0.8311976572447164 \tvalid loss: 0.08860086359580358 accuracy 0.8176937950937949\n",
            "epoch:19/20 train loss: 0.07129977973535949 accuracy: 0.8361395127748065 \tvalid loss: 0.08487804748614629 accuracy 0.8219244829244828\n",
            "epoch:20/20 train loss: 0.06833600648477965 accuracy: 0.8447356643356642 \tvalid loss: 0.08507015094492171 accuracy 0.8146228635561968\n",
            "n_reduce: 16\n",
            "N_fc_layers 0\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.16484399690464432 accuracy: 0.6418833828480888 \tvalid loss: 0.12405846603711446 accuracy 0.6752869488536155\n",
            "epoch:2/20 train loss: 0.10669381151830448 accuracy: 0.7256525337407687 \tvalid loss: 0.10339456029070748 accuracy 0.7444964405964407\n",
            "epoch:3/20 train loss: 0.09489472232262293 accuracy: 0.7731646125116711 \tvalid loss: 0.09628104523817699 accuracy 0.7914237133237133\n",
            "epoch:4/20 train loss: 0.08973254570423388 accuracy: 0.7926777000777004 \tvalid loss: 0.09519029191467497 accuracy 0.7742650793650793\n",
            "epoch:5/20 train loss: 0.08881130735897551 accuracy: 0.7983821633703987 \tvalid loss: 0.09580019891924328 accuracy 0.7869803270803271\n",
            "epoch:6/20 train loss: 0.08719111888665779 accuracy: 0.8021713521772348 \tvalid loss: 0.10475896179676056 accuracy 0.7832447971781304\n",
            "epoch:7/20 train loss: 0.08611075630141239 accuracy: 0.8086786662139603 \tvalid loss: 0.09446569007635117 accuracy 0.7999615520282186\n",
            "epoch:8/20 train loss: 0.08220384890659183 accuracy: 0.8132222703222698 \tvalid loss: 0.09173454942968157 accuracy 0.8056825076158409\n",
            "epoch:9/20 train loss: 0.08004599584902033 accuracy: 0.8179605834252898 \tvalid loss: 0.09041360657082663 accuracy 0.7922650793650794\n",
            "epoch:10/20 train loss: 0.07822430921535865 accuracy: 0.8225674943792592 \tvalid loss: 0.09001833829614851 accuracy 0.7966590989257656\n",
            "epoch:11/20 train loss: 0.07784554915802151 accuracy: 0.8220872393401807 \tvalid loss: 0.08958671375115712 accuracy 0.7979964405964406\n",
            "epoch:12/20 train loss: 0.07824686273523405 accuracy: 0.8202396231219763 \tvalid loss: 0.08955924057298237 accuracy 0.8070804393137728\n",
            "epoch:13/20 train loss: 0.07952289342062147 accuracy: 0.8194561003266886 \tvalid loss: 0.09080768885877398 accuracy 0.7953197530864198\n",
            "epoch:14/20 train loss: 0.08090013740109463 accuracy: 0.8176520810344338 \tvalid loss: 0.0903622333407402 accuracy 0.8037386884720217\n",
            "epoch:15/20 train loss: 0.08203290203739615 accuracy: 0.8138368970898384 \tvalid loss: 0.0940890492796898 accuracy 0.8025196568863235\n",
            "epoch:16/20 train loss: 0.08165030105674968 accuracy: 0.8167202969144147 \tvalid loss: 0.0960582946870062 accuracy 0.7876448773448774\n",
            "epoch:17/20 train loss: 0.08072908984446057 accuracy: 0.8181897065897064 \tvalid loss: 0.09820376753144794 accuracy 0.8012300304633637\n",
            "epoch:18/20 train loss: 0.07806903245051702 accuracy: 0.8223932065755595 \tvalid loss: 0.09585446211364534 accuracy 0.7961125380792046\n",
            "epoch:19/20 train loss: 0.07573235999602898 accuracy: 0.8276939167586224 \tvalid loss: 0.0914089378979471 accuracy 0.8098484046817379\n",
            "epoch:20/20 train loss: 0.07420953239938792 accuracy: 0.8294889850889852 \tvalid loss: 0.09007291773292754 accuracy 0.8044805034471701\n",
            "n_reduce: 16\n",
            "N_fc_layers 1\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "epoch:1/20 train loss: 0.3886081153388117 accuracy: 0.5786826069897915 \tvalid loss: 0.11319336499770483 accuracy 0.7689190796857464\n",
            "epoch:2/20 train loss: 0.097938870241829 accuracy: 0.7866792181674533 \tvalid loss: 0.09153955066866346 accuracy 0.7797409010742343\n",
            "epoch:3/20 train loss: 0.08956588883259717 accuracy: 0.7967491399667869 \tvalid loss: 0.09053348569075266 accuracy 0.7846311207311206\n",
            "epoch:4/20 train loss: 0.08695154398679733 accuracy: 0.8024337209631327 \tvalid loss: 0.08941728752189212 accuracy 0.794285137085137\n",
            "epoch:5/20 train loss: 0.08548623245603898 accuracy: 0.8052405058993297 \tvalid loss: 0.09050367483827802 accuracy 0.8028625460958795\n",
            "epoch:6/20 train loss: 0.08407370684427373 accuracy: 0.8094330915272091 \tvalid loss: 0.09068460694286558 accuracy 0.8090025493025493\n",
            "epoch:7/20 train loss: 0.08194529282579235 accuracy: 0.8143033754916108 \tvalid loss: 0.09058562239011128 accuracy 0.8073368446368446\n",
            "epoch:8/20 train loss: 0.0794091579212862 accuracy: 0.8189416291769233 \tvalid loss: 0.08767519954178069 accuracy 0.8011329805996473\n",
            "epoch:9/20 train loss: 0.07663936838799831 accuracy: 0.8256731347084285 \tvalid loss: 0.08704912394947475 accuracy 0.8080029982363316\n",
            "epoch:10/20 train loss: 0.0738685136308857 accuracy: 0.8337103273632689 \tvalid loss: 0.08609930855698056 accuracy 0.8078128427128427\n",
            "epoch:11/20 train loss: 0.07214989971530204 accuracy: 0.8374988795518208 \tvalid loss: 0.08747009544240104 accuracy 0.8170185986852654\n",
            "epoch:12/20 train loss: 0.07219007160149368 accuracy: 0.8367617147558323 \tvalid loss: 0.08673634337054359 accuracy 0.8103098925765593\n",
            "epoch:13/20 train loss: 0.0745955633591203 accuracy: 0.8303905440964267 \tvalid loss: 0.08883372090922462 accuracy 0.8179665223665223\n",
            "epoch:14/20 train loss: 0.07733886935664158 accuracy: 0.8234867324396737 \tvalid loss: 0.09132363713449902 accuracy 0.8009184223184223\n",
            "epoch:15/20 train loss: 0.07901453767453923 accuracy: 0.8198293453822861 \tvalid loss: 0.0910372444987297 accuracy 0.8125036075036074\n",
            "epoch:16/20 train loss: 0.07875911731579724 accuracy: 0.8202619075913192 \tvalid loss: 0.08824658497174581 accuracy 0.807574202340869\n",
            "epoch:17/20 train loss: 0.07729465867374458 accuracy: 0.8229198483433778 \tvalid loss: 0.09158947994973925 accuracy 0.7921385121051788\n",
            "epoch:18/20 train loss: 0.07525430402802485 accuracy: 0.8273355087178613 \tvalid loss: 0.08845584436257681 accuracy 0.8087019400352733\n",
            "epoch:19/20 train loss: 0.07212809425592423 accuracy: 0.835287177093059 \tvalid loss: 0.08755311158630583 accuracy 0.8141669713003045\n",
            "epoch:20/20 train loss: 0.0689194389815424 accuracy: 0.8431124041535807 \tvalid loss: 0.08597020222743353 accuracy 0.8127036876703544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW9tT9lJDP2z"
      },
      "source": [
        "res50.save(\"save1\")\n",
        "#files.download('my_model_save1.pth')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTWNjSmSztXC"
      },
      "source": [
        "#res50 = resnet_50(N_classes=20, dropout_p=0.8, N_fc_layers=2)\n",
        "#test_set = TestDataloader(image_infos, data_root_dir).make_loader(batch_size = 128)\n",
        "#res50.predict(test_set)\n",
        "def load_model(file):\n",
        "    return pytorch.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}