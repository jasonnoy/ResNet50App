{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5329assn2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUOYv9U1H3TS1jHDCq1LJs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonnoy/COMP5329NEW/blob/best_model/5329assn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P22itkey3yuS",
        "outputId": "936af3be-b585-4333-ccdb-0485ffd9004c"
      },
      "source": [
        "# mount google drive to colabif not mounted before\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbqypOO10hvP"
      },
      "source": [
        "# import necessary library\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "\n",
        "DEBUG = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "61DjxixN3QLF",
        "outputId": "9e285b7d-b9e3-4fa4-bb83-94a0e4259989"
      },
      "source": [
        "# download data from kaggle server with token file\n",
        "# must update before use in Colab!\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "# following method of download dataset is from kaggle official forum : https://www.kaggle.com/general/74235\n",
        "from google.colab import files\n",
        "print(\"Please upload your kaggle.jasn token file, to allow direct file download from kaggle to Colab\")\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c '2021s1comp5329assignment2'\n",
        "! unzip -q 2021s1comp5329assignment2.zip\n",
        "! rm 2021s1comp5329assignment2.zip\n",
        "\n",
        "data_root_dir = \"/content/COMP5329S1A2Dataset/data/\"\n",
        "print(\"DONE! image files in : \",data_root_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=01229d17f8461dfe8d176b3fbe460a18364206e63388144e2ee17853f5b523fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n",
            "Please upload your kaggle.jasn token file, to allow direct file download from kaggle to Colab\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f2c8fd73-9c3c-4789-af29-bec9b496c680\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f2c8fd73-9c3c-4789-af29-bec9b496c680\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading 2021s1comp5329assignment2.zip to /content\n",
            " 98% 1.29G/1.31G [00:10<00:00, 131MB/s]\n",
            "100% 1.31G/1.31G [00:10<00:00, 138MB/s]\n",
            "DONE! image files in :  /content/COMP5329S1A2Dataset/data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "BZH1TndQ5PG5",
        "outputId": "1f336224-8f4a-40a0-9410-ffc43a961f17"
      },
      "source": [
        "print(\"Please upload fixed train.csv and test.csv\")\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "train_csv=pd.read_csv(\"train.csv\")\n",
        "label_column=train_csv['Labels']\n",
        "img_column=train_csv['ImageID']\n",
        "print(label_column)\n",
        "\n",
        "test_csv=pd.read_csv(\"test.csv\")\n",
        "test_column=test_csv['ImageID']\n",
        "print(test_column)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please upload fixed train.csv and test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bbbc2b4c-b8a1-4a9b-b738-86c76e89c7a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bbbc2b4c-b8a1-4a9b-b738-86c76e89c7a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n",
            "0             1\n",
            "1          1 19\n",
            "2             1\n",
            "3        8 3 13\n",
            "4         8 3 7\n",
            "          ...  \n",
            "29995     8 1 2\n",
            "29996         1\n",
            "29997         1\n",
            "29998         1\n",
            "29999         1\n",
            "Name: Labels, Length: 30000, dtype: object\n",
            "0       30000.jpg\n",
            "1       30001.jpg\n",
            "2       30002.jpg\n",
            "3       30003.jpg\n",
            "4       30004.jpg\n",
            "          ...    \n",
            "9995    39995.jpg\n",
            "9996    39996.jpg\n",
            "9997    39997.jpg\n",
            "9998    39998.jpg\n",
            "9999    39999.jpg\n",
            "Name: ImageID, Length: 10000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QHkkStK6ZsE",
        "outputId": "11bc04b6-5997-4d58-9fb4-b812a7f098e3"
      },
      "source": [
        "# create colums for each class\n",
        "col_list=np.arange(20)\n",
        "print(col_list)\n",
        "\n",
        "# create zeros in np with number of sample from train.csv\n",
        "labels=pd.DataFrame(np.zeros((len(train_csv),20), dtype=np.int), columns=col_list)\n",
        "print(\"zeros shape :\",labels.shape)\n",
        "\n",
        "# change lables matrix value for all samples with split\n",
        "index=0\n",
        "for row in label_column:\n",
        "    label_list=list(map(int, row.split(\" \")))\n",
        "    for i in label_list:\n",
        "        labels.iloc[index,i] = 1\n",
        "    index+=1\n",
        "print(labels)\n",
        "\n",
        "# combine image file name and label matrix\n",
        "# and store this dataframe to file\n",
        "processed_train_set = pd.concat([img_column, labels], axis=1)\n",
        "print(processed_train_set)\n",
        "processed_train_set.to_csv(\"processed_train.csv\",index=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "zeros shape : (30000, 20)\n",
            "       0   1   2   3   4   5   6   7   8   ...  11  12  13  14  15  16  17  18  19\n",
            "0       0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "1       0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   1\n",
            "2       0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "3       0   0   0   1   0   0   0   0   1  ...   0   0   1   0   0   0   0   0   0\n",
            "4       0   0   0   1   0   0   0   1   1  ...   0   0   0   0   0   0   0   0   0\n",
            "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "29995   0   1   1   0   0   0   0   0   1  ...   0   0   0   0   0   0   0   0   0\n",
            "29996   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "29997   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "29998   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "29999   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "\n",
            "[30000 rows x 20 columns]\n",
            "         ImageID  0  1  2  3  4  5  6  7  ...  11  12  13  14  15  16  17  18  19\n",
            "0          0.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "1          1.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   1\n",
            "2          2.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "3          3.jpg  0  0  0  1  0  0  0  0  ...   0   0   1   0   0   0   0   0   0\n",
            "4          4.jpg  0  0  0  1  0  0  0  1  ...   0   0   0   0   0   0   0   0   0\n",
            "...          ... .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "29995  29995.jpg  0  1  1  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29996  29996.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29997  29997.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29998  29998.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "29999  29999.jpg  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0\n",
            "\n",
            "[30000 rows x 21 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ZHGBx4UUAR2k",
        "outputId": "2f17c449-4851-402c-9b1a-db2bb02766ed"
      },
      "source": [
        "# class distribution\n",
        "class_sum = labels.sum(axis=0)\n",
        "class_percentage = class_sum/class_sum.sum(axis=0)\n",
        "for it in col_list:\n",
        "    print(\"%3d\" %it,end='\\t')\n",
        "print()\n",
        "for it in col_list:\n",
        "    print(\"%.3f\" %class_percentage[it],end='\\t')\n",
        "\n",
        "# plot class distribution\n",
        "fig = plt.figure(figsize =(10, 6))\n",
        "plt.bar(col_list, class_percentage)\n",
        "plt.xticks(col_list, col_list)\n",
        "plt.show()\n",
        "\n",
        "# image dataset statistics\n",
        "image_infos = []\n",
        "tran_stat = transforms.Compose([transforms.ToTensor()])\n",
        "for it in processed_train_set.ImageID:\n",
        "    image = np.array(cv2.cvtColor(cv2.imread(data_root_dir+it), cv2.COLOR_BGR2RGB))\n",
        "    curr_infos = []\n",
        "    # image N rows and N cols\n",
        "    curr_infos.append(image.shape[0])\n",
        "    curr_infos.append(image.shape[1])\n",
        "    image = tran_stat(image)\n",
        "    # image RGB value means\n",
        "    curr_infos.append(image[:,:,0].mean())\n",
        "    curr_infos.append(image[:,:,1].mean())\n",
        "    curr_infos.append(image[:,:,2].mean())\n",
        "    # image RGB value stds\n",
        "    curr_infos.append(np.sqrt(image[:,:,0].var()+1e-5))\n",
        "    curr_infos.append(np.sqrt(image[:,:,1].var()+1e-5))\n",
        "    curr_infos.append(np.sqrt(image[:,:,2].var()+1e-5))\n",
        "    image_infos.append(curr_infos)\n",
        "\n",
        "image_infos = np.array(image_infos)\n",
        "print(\"\\t row \\t col \\t RGB means \\t RGB stds\")\n",
        "image_infos = image_infos.mean(axis=0)\n",
        "print(image_infos)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0\t  1\t  2\t  3\t  4\t  5\t  6\t  7\t  8\t  9\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t\n",
            "0.000\t0.490\t0.025\t0.094\t0.027\t0.024\t0.030\t0.026\t0.047\t0.022\t0.032\t0.013\t0.000\t0.013\t0.005\t0.042\t0.024\t0.031\t0.033\t0.022\t"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\t row \t col \t RGB means \t RGB stds\n",
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjS6sdswO-8y"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_infos, data_root_dir=data_root_dir, csv_name=\"processed_train.csv\"):\n",
        "        self.image_infos = image_infos\n",
        "        print(self.image_infos)\n",
        "        self.data_root_dir = data_root_dir\n",
        "        self.csv_file_label = pd.read_csv(csv_name)\n",
        "        self.transforms = transforms.Compose([transforms.Resize((int(self.image_infos[0]),int(self.image_infos[1]))) , \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([self.image_infos[2], self.image_infos[3], self.image_infos[4]], \n",
        "                                                                 [self.image_infos[5], self.image_infos[6], self.image_infos[7]])])\n",
        "        #self.transforms = transforms.Compose([transforms.Resize((244,244)) , \n",
        "        #                       transforms.ToTensor(),\n",
        "        #                       transforms.Normalize([0.4, 0.4, 0.4], [0.2, 0.2, 0.2])\n",
        "        #                       ])\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file_label)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #image = cv2.cvtColor(cv2.imread(self.data_root_dir + self.csv_file_label.iloc[index].ImageID), cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(self.data_root_dir + self.csv_file_label.iloc[index].ImageID).convert(\"RGB\")\n",
        "        label = torch.tensor(self.csv_file_label.iloc[index][1:].tolist(), dtype=torch.float32)\n",
        "        \n",
        "        return self.transforms(image), label\n",
        "\n",
        "class ImageDataloader():\n",
        "    def __init__(self, image_infos, data_root_dir):\n",
        "        train_percentage = 0.85\n",
        "        image_dataset = ImageDataset(image_infos, data_root_dir, \"processed_train.csv\")\n",
        "        dataset_len = len(image_dataset)\n",
        "        self.train_set, self.valid_set = random_split(image_dataset, [int(dataset_len*train_percentage), (dataset_len-int(dataset_len*train_percentage))])\n",
        "\n",
        "    def make_loader(self, batch_size):\n",
        "        return DataLoader(self.train_set, shuffle=True, batch_size=batch_size), DataLoader(self.valid_set, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIkWkSOkJfQ"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_infos, data_root_dir=data_root_dir, csv_name=\"test.csv\"):\n",
        "        self.image_infos = image_infos\n",
        "        print(self.image_infos)\n",
        "        self.data_root_dir = data_root_dir\n",
        "        self.csv_file_label = pd.read_csv(csv_name)\n",
        "        self.transforms = transforms.Compose([transforms.Resize((int(self.image_infos[0]),int(self.image_infos[1]))) , \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([self.image_infos[2], self.image_infos[3], self.image_infos[4]], \n",
        "                                                                 [self.image_infos[5], self.image_infos[6], self.image_infos[7]])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file_label)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #image = cv2.cvtColor(cv2.imread(self.data_root_dir + self.csv_file_label.iloc[index].ImageID), cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(self.data_root_dir + self.csv_file_label.iloc[index].ImageID).convert(\"RGB\")\n",
        "        image_ID = self.csv_file_label.iloc[index].ImageID\n",
        "        \n",
        "        return self.transforms(image), image_ID\n",
        "\n",
        "class TestDataloader():\n",
        "    def __init__(self, image_infos, data_root_dir):\n",
        "        self.test_set = TestDataset(image_infos, data_root_dir, \"test.csv\")\n",
        "\n",
        "    def make_loader(self, batch_size):\n",
        "        return DataLoader(self.test_set, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTKmwC3RFcbH"
      },
      "source": [
        "import torch.nn as nn\n",
        "class AsymmetricLoss(nn.Module):\n",
        "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
        "        super(AsymmetricLoss, self).__init__()\n",
        "\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.clip = clip\n",
        "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: input logits\n",
        "        y: targets (multi-label binarized vector)\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculating Probabilities\n",
        "        x_sigmoid = torch.sigmoid(x)\n",
        "        xs_pos = x_sigmoid\n",
        "        xs_neg = 1 - x_sigmoid\n",
        "\n",
        "        # Asymmetric Clipping\n",
        "        if self.clip is not None and self.clip > 0:\n",
        "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
        "\n",
        "        # Basic CE calculation\n",
        "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
        "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
        "        loss = los_pos + los_neg\n",
        "\n",
        "        # Asymmetric Focusing\n",
        "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(False)\n",
        "            pt0 = xs_pos * y\n",
        "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
        "            pt = pt0 + pt1\n",
        "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
        "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(True)\n",
        "            loss *= one_sided_w\n",
        "\n",
        "        return -loss.sum()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdkFr2rTO0Q2"
      },
      "source": [
        "class resnet_50:\n",
        "    def __init__(self, N_classes=20, dropout_p=0.5, N_fc_layers=3):\n",
        "        \"\"\"\n",
        "        :type dropout_p: float\n",
        "        :param: dropout_p: the chance of nodes stay in the network\n",
        "        \"\"\"\n",
        "        #check device, if GPU avaliable for torch\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "\n",
        "        #self.model = models.resnet50(pretrained=True)\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.N_classes = N_classes\n",
        "        # decide is need dropout, base on dropout_p value\n",
        "        # if dropout_p == 1, all node preserved, so no dropout\n",
        "        self.dropout_p = dropout_p\n",
        "        self.dropout = False\n",
        "        if self.dropout_p != 1.0:\n",
        "            self.dropout = True\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # connect Resnet50 to N_classes output fc\n",
        "        self.activ_func = nn.ReLU\n",
        "        N_fc_in = self.model.fc.in_features\n",
        "        self.N_fc_layers = N_fc_layers\n",
        "        fc_layers = []\n",
        "\n",
        "        n_reduce = 4\n",
        "\n",
        "        if self.dropout:\n",
        "            for N_it in range(self.N_fc_layers):\n",
        "                fc_layers.append(nn.Linear(N_fc_in//(n_reduce**N_it), N_fc_in//(n_reduce**(N_it+1))))\n",
        "                fc_layers.append(nn.ReLU())\n",
        "                fc_layers.append(nn.BatchNorm1d(N_fc_in//(n_reduce**(N_it+1))))\n",
        "                # nn.dropout use probability of dropout as input\n",
        "                fc_layers.append(nn.Dropout(1-self.dropout_p))\n",
        "        else:\n",
        "            for N_it in range(self.N_fc_layers):\n",
        "                fc_layers.append(nn.Linear(N_fc_in//(n_reduce**N_it), N_fc_in//(n_reduce**(N_it+1))))\n",
        "                fc_layers.append(nn.ReLU())\n",
        "                fc_layers.append(nn.BatchNorm1d(N_fc_in//(n_reduce**(N_it+1))))\n",
        "        \n",
        "        # last layer to connect to N_classes\n",
        "        fc_layers.append(nn.Linear(N_fc_in//(n_reduce**(self.N_fc_layers)), self.N_classes))\n",
        "        # add custom fc layers to Resnet\n",
        "        self.model.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "        if DEBUG:\n",
        "            print(self.model)\n",
        "\n",
        "        # change device\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        #define the min threshold for prediction\n",
        "        self.predict_threshold = 0.5\n",
        "\n",
        "    def accuracy_calculate(self, y_true, outputs_torch, curr_batch_size):\n",
        "        y_true = y_true.to(torch.int).numpy()\n",
        "        y_pred = (torch.sigmoid(outputs_torch).data > self.predict_threshold).cpu().detach().to(torch.int).numpy()\n",
        "        return f1_score(y_true, y_pred, average='samples')*curr_batch_size\n",
        "\n",
        "    def train(self, train_dataloader, valid_dataloader, N_epochs=10, learning_rate=0.01, optimizer=\"adam\", scheduler=\"steplr\"):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.len_train = len(train_dataloader.dataset)\n",
        "        self.len_valid = len(valid_dataloader.dataset)\n",
        "\n",
        "        self.optimizer = None\n",
        "        # add training optimizers\n",
        "        if optimizer == \"adamw\":\n",
        "            print(\"optimizer : AdamW\")\n",
        "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "        elif optimizer == \"sparseadam\":\n",
        "            print(\"optimizer : SparseAdam\")\n",
        "            self.optimizer = torch.optim.SparseAdam(self.model.parameters(), lr=self.learning_rate)\n",
        "        else:\n",
        "            print(\"optimizer : Adam\")\n",
        "            # default as Adam\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # add training schedulers\n",
        "        self.scheduler = None\n",
        "        if scheduler == \"reducelr\":\n",
        "            print(\"scheduler : ReduceLROnPlateau\")\n",
        "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "        elif scheduler == \"cosinelr\":\n",
        "            print(\"scheduler : CosineAnnealingLR\")\n",
        "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=5, eta_min=0.005)\n",
        "        else:\n",
        "            print(\"scheduler : StepLR\")\n",
        "            # default as steplr\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # add training optimizaers\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        #self.criterion = AsymmetricLoss()\n",
        "\n",
        "        # recode evaluation matries for each epoch\n",
        "        #============================train==============================\n",
        "        for epoch_it in range(N_epochs):\n",
        "            #epoch loop\n",
        "            print(\"\\nepoch:\", (epoch_it+1), '/', N_epochs,end=' ')\n",
        "            train_accuracy = 0.0 \n",
        "            train_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for images, labels in train_dataloader:\n",
        "                torch.set_grad_enabled(True)\n",
        "                # batch loop\n",
        "                images = images.to(self.device)\n",
        "                labels_processed = labels.to(self.device)\n",
        "                curr_batch_size = images.size(0)\n",
        "                \n",
        "                # get output and calculate accuracy\n",
        "                outputs = self.model(images)\n",
        "                train_accuracy += self.accuracy_calculate(labels, outputs, curr_batch_size)\n",
        "\n",
        "                # loss & backpropergate\n",
        "                loss = self.criterion(outputs, labels_processed)\n",
        "                loss.backward()\n",
        "                train_loss += curr_batch_size*loss.item()\n",
        "\n",
        "                # update parameters by grads & reset grads\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "            \n",
        "            # train print out\n",
        "            print(\"train loss:\", (train_loss/self.len_train),\n",
        "                  \"accuracy:\", (train_accuracy/self.len_train),end=' ')\n",
        "\n",
        "            #============================valid==============================\n",
        "            valid_accuracy = 0.0\n",
        "            valid_loss = 0.0\n",
        "            self.model.eval()\n",
        "       \n",
        "            for images, labels in valid_dataloader:\n",
        "                torch.set_grad_enabled(False)\n",
        "                # batch loop\n",
        "                images = images.to(self.device)\n",
        "                labels_processed = labels.to(self.device)\n",
        "                curr_batch_size = images.size(0)\n",
        "                \n",
        "                # get output and calculate accuracy\n",
        "                outputs = self.model(images)\n",
        "                valid_accuracy += self.accuracy_calculate(labels, outputs, curr_batch_size)\n",
        "\n",
        "                # loss & backpropergate\n",
        "                loss = self.criterion(outputs, labels_processed)\n",
        "                valid_loss += curr_batch_size*loss.item()\n",
        "\n",
        "            # valid print out\n",
        "            print(\"\\tvalid loss:\", (valid_loss/self.len_valid),\n",
        "                  \"accuracy\", (valid_accuracy/self.len_valid),end='')\n",
        "            \n",
        "            self.scheduler.step()\n",
        "\n",
        "    def test_pred(self, outputs_torch, curr_batch_size):\n",
        "        y_pred = (torch.sigmoid(outputs_torch).data > self.predict_threshold).cpu().detach().to(torch.int).numpy()\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, test_dataloader, output_file_name='Predicted_labels.txt'):\n",
        "    #============================test==============================\n",
        "        self.model.eval()\n",
        "        output_file = open(output_file_name,\"w+\")\n",
        "\n",
        "        for images, image_ID in test_dataloader:\n",
        "            torch.set_grad_enabled(False)\n",
        "            # batch loop\n",
        "            images = images.to(self.device)\n",
        "            curr_batch_size = images.size(0)\n",
        "            \n",
        "            # get output and calculate accuracy\n",
        "            outputs = self.model(images)\n",
        "\n",
        "            y_preds = self.test_pred(outputs, curr_batch_size)\n",
        "            output_file.write(\"ImageID,Labels\")\n",
        "\n",
        "            for it in range(len(y_preds)):\n",
        "                output_file.write(\"%s,\"%image_ID[it])\n",
        "\n",
        "                pred_label = np.atleast_1d(np.squeeze(np.argwhere(y_preds[it]==1)))\n",
        "                for label_it in pred_label:\n",
        "                    output_file.write(\" %d\"%label_it)\n",
        "                output_file.write('\\n')\n",
        "\n",
        "        output_file.close()        \n",
        "\n",
        "    def save(self, name):\n",
        "        torch.save(self.model, \"my_model_\"+name+\".pth\")\n",
        "\n",
        "    def load(self, file):\n",
        "        self.model = torch.load(file)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw0v6cm00PY1"
      },
      "source": [
        "#DEBUG = False\n",
        "\n",
        "#for dropout_p in [0.8]:\n",
        "#    print(\"\\n===============dropout_p:\",dropout_p)\n",
        "#    for optimizer in [\"adam\", \"adamw\"]:\n",
        "#        res50 = resnet_50(N_classes=20, dropout_p=dropout_p, N_fc_layers=3)\n",
        "\n",
        "#        dataset_loader = ImageDataloader(image_infos, data_root_dir)\n",
        "#        (train_set, valid_set) = dataset_loader.make_loader(batch_size = 128)\n",
        "#        res50.train(train_dataloader=train_set, valid_dataloader=valid_set, N_epochs=20, learning_rate=0.001, optimizer=optimizer, scheduler=\"cosinelr\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmkgADj4lr3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "fa63e846-089b-4c23-fd1f-5190d1bdaa62"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "res50 = resnet_50(N_classes=20, dropout_p=0.8, N_fc_layers=0)\n",
        "\n",
        "dataset_loader = ImageDataloader(image_infos, data_root_dir)\n",
        "(train_set, valid_set) = dataset_loader.make_loader(batch_size = 128)\n",
        "res50.train(train_dataloader=train_set, valid_dataloader=valid_set, N_epochs=20, learning_rate=0.001, optimizer=\"adam\", scheduler=\"cosinelr\")\n",
        "\n",
        "test_set = TestDataloader(image_infos, data_root_dir).make_loader(batch_size = 128)\n",
        "res50.predict(test_set)\n",
        "files.download('Predicted_labels.txt')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n",
            "optimizer : Adam\n",
            "scheduler : CosineAnnealingLR\n",
            "\n",
            "epoch: 1 / 20 train loss: 0.1392313442440594 accuracy: 0.6771448963890139 \tvalid loss: 0.10306392529937956 accuracy 0.7412309443642778\n",
            "epoch: 2 / 20 train loss: 0.09815174355343276 accuracy: 0.7626992077639133 \tvalid loss: 0.09287991993957095 accuracy 0.7876773769440436\n",
            "epoch: 3 / 20 train loss: 0.09191154973004378 accuracy: 0.7881262428203603 \tvalid loss: 0.0911319389210807 accuracy 0.8021426487093154\n",
            "epoch: 4 / 20 train loss: 0.09108457069770963 accuracy: 0.7946339812692752 \tvalid loss: 0.09581709576977623 accuracy 0.8056488856822192\n",
            "epoch: 5 / 20 train loss: 0.09164372364446229 accuracy: 0.7956343236284416 \tvalid loss: 0.0990944181614452 accuracy 0.8065385922719257\n",
            "epoch: 6 / 20 train loss: 0.09013619848092397 accuracy: 0.8014935008346773 \tvalid loss: 0.09259938875834148 accuracy 0.7908344717011386\n",
            "epoch: 7 / 20 train loss: 0.08817183021587484 accuracy: 0.8047360086536556 \tvalid loss: 0.09641365933418274 accuracy 0.8097288600288598\n",
            "epoch: 8 / 20 train loss: 0.08633326397806991 accuracy: 0.8080343264578557 \tvalid loss: 0.09234618782997131 accuracy 0.8036963283629951\n",
            "epoch: 9 / 20 train loss: 0.0805299722014689 accuracy: 0.8177608578785053 \tvalid loss: 0.08834246184428533 accuracy 0.8057073112073113\n",
            "epoch: 10 / 20 train loss: 0.07766196765619165 accuracy: 0.8219700761112526 \tvalid loss: 0.08869203595320384 accuracy 0.8027425845759179\n",
            "epoch: 11 / 20 train loss: 0.07652601593265347 accuracy: 0.8237061097943452 \tvalid loss: 0.08645537837346395 accuracy 0.815803334936668\n",
            "epoch: 12 / 20 train loss: 0.077507005503949 accuracy: 0.8213720397249811 \tvalid loss: 0.08688609890143077 accuracy 0.8073205066538398\n",
            "epoch: 13 / 20 train loss: 0.07965077380280869 accuracy: 0.8200086749851454 \tvalid loss: 0.0907909271452162 accuracy 0.7948190796857464\n",
            "epoch: 14 / 20 train loss: 0.08358201175928116 accuracy: 0.8149009054126701 \tvalid loss: 0.09149334827396605 accuracy 0.8097348725348725\n",
            "epoch: 15 / 20 train loss: 0.08476464480629155 accuracy: 0.8139259400729983 \tvalid loss: 0.11061287447479036 accuracy 0.7546550745550745\n",
            "epoch: 16 / 20 train loss: 0.08620626396875755 accuracy: 0.8113528634763931 \tvalid loss: 0.09289347012837727 accuracy 0.8157028699695368\n",
            "epoch: 17 / 20 train loss: 0.08538989007122376 accuracy: 0.8142187109187111 \tvalid loss: 0.10343741757339901 accuracy 0.799094324194324\n",
            "epoch: 18 / 20 train loss: 0.07996899264115913 accuracy: 0.8225854252501303 \tvalid loss: 0.09547631262408363 accuracy 0.7750697290363956\n",
            "epoch: 19 / 20 train loss: 0.07581181546169169 accuracy: 0.8274398947457774 \tvalid loss: 0.09024304236968358 accuracy 0.8193144139810806\n",
            "epoch: 20 / 20 train loss: 0.07276218471573849 accuracy: 0.8333161785926493 \tvalid loss: 0.08799282933606042 accuracy 0.803613949013949[2.40547633e+02 2.89959400e+02 4.21987559e-01 4.23397052e-01\n",
            " 4.25649481e-01 1.89648016e-01 1.91516552e-01 1.92970209e-01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_abeb7acb-def6-4247-8a19-6525816b0daf\", \"Predicted_labels.txt\", 138112)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW9tT9lJDP2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b02469e4-e630-49ac-d027-6b11acdc2b2b"
      },
      "source": [
        "res50.save(\"save1\")\n",
        "files.download('my_model_save1.pth')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_94c7ede4-4f31-4be8-97f0-dd6c854606cb\", \"my_model_save1.pth\", 96500782)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTWNjSmSztXC"
      },
      "source": [
        "#res50 = resnet_50(N_classes=20, dropout_p=0.8, N_fc_layers=2)\n",
        "#test_set = TestDataloader(image_infos, data_root_dir).make_loader(batch_size = 128)\n",
        "#res50.predict(test_set)\n",
        "def load_model(file):\n",
        "    return pytorch.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}